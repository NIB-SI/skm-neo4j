{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install monotonic openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:80% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import neo4j DB: 1/?\n",
    "\n",
    "Code to translate v2.7.4_PIS-model.xlsx to neo4j database. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "import os\n",
    "from IPython.display import Image, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_labels = helpers.node_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in sheets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "base_path = Path(\"..\")\n",
    "input_path = base_path / \"data\" / \"raw\"\n",
    "output_path = base_path / \"data\" / \"parsed\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Components sheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sheets = [#(file, sheet_name)]\n",
    "    (\"v2.7.9_PIS-model.xlsx\", \"Components\"),\n",
    "#    (\"v2.7.5_PIS-model.xlsx\", \"Components_New\"), \n",
    "#    (\"Model_CK.xlsx\", \"Components_new\"), \n",
    "#    (\"v2.7.2_PIS-model-JALR.xlsx\", \"Components_New\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resave xlsx as tsv\n",
    "drops = ['Legacy:Process', 'Notes', 'Unnamed: 21', 'GMM_OCD', 'ExternalDB', 'Notes', 'GMM:Synonyms.1']\n",
    "col_rename = {\n",
    "    'mID':'identifier',\n",
    "    'AddedBy':'AddedBy', \n",
    "    'Species':'Species', \n",
    "    'NodeType':'NodeType', \n",
    "    'Family':'Family', \n",
    "    'Clade':'Clade', \n",
    "    'NodeID':'NodeID', \n",
    "    'NodeName':'NodeName', \n",
    "    'ModelStatus':'ModelStatus', \n",
    "    'NodeDescription':'NodeDescription', \n",
    "    'AdditionalInfo':'AdditionalInfo', \n",
    "    'Process':'Process', \n",
    "    'ModelV':'ModelV', \n",
    "\n",
    "    'ExtDBlink':'ExtDBlink', \n",
    "\n",
    "    'GMM_OCD1':'GMM_OCD', \n",
    "    'GMM:Description':'GMM_Description', \n",
    "    'GMM:ShortName':'GMM_ShortName', \n",
    "    'GMM:Synonyms':'synonyms', \n",
    "\n",
    "    'Node':'NodeName'\n",
    "}\n",
    "\n",
    "for file_name, sheet_name in sheets:   \n",
    "    file_path = input_path / file_name\n",
    "    base_name = file_path.stem\n",
    "    \n",
    "    new_file_path = output_path / f'{base_name}-{sheet_name}.tsv'\n",
    "    \n",
    "    print(f\"{file_name} sheet '{sheet_name}' will be saved to '{new_file_path}'\")\n",
    "#    if os.path.exists(new_file_path):\n",
    "#        continue\n",
    "        \n",
    "    df = pd.read_excel(file_path, \n",
    "                    sheet_name=sheet_name, \n",
    "                    header=[1], \n",
    "                    dtype=str, \n",
    "                    na_values=helpers.empty_strings)\n",
    "    \n",
    "    to_drop = list(set(drops) & set(df.columns)) + list(df.filter(regex=(\"Unnamed.*\")).columns)\n",
    "    df.drop(to_drop, axis=1, inplace=True)\n",
    "    \n",
    "    new_cols = [col_rename[x] for x in df.columns]\n",
    "    df.columns = new_cols\n",
    "    \n",
    "    df.to_csv(new_file_path, sep=\"\\t\", index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = []\n",
    "\n",
    "for file_name, sheet_name in sheets:\n",
    "    print(file_name, sheet_name)\n",
    "    base_name, extension = os.path.splitext(file_name)\n",
    "    file_path = output_path / f'{base_name}-{sheet_name}.tsv'\n",
    "    df = pd.read_csv(file_path, sep=\"\\t\")\n",
    "    dfs.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_components = pd.concat(dfs, sort=False)\n",
    "df_components.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.value_counts(df_components['NodeType'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_components.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in df_components.columns:\n",
    "    df_components[c] = df_components[c].str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_components[df_components['NodeType']=='x']\n",
    "# x = df_components[df_components['NodeType']=='x'].index; display(x)\n",
    "# df_components.drop(x, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_components.loc[df_components[\"NodeName\"].isna(), 'NodeName']\n",
    "#df_components.loc[df_components[\"NodeName\"].isna(), 'NodeName'] =  df_components.loc[df_components[\"NodeName\"].isna(), 'NodeID']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "components_node_type_to_node_label = {\n",
    "    \"plant coding\":\"PlantCoding\",\n",
    "    \"plant_coding\":\"PlantCoding\",\n",
    "    \"plant_noncoding\":\"PlantNonCoding\",\n",
    "    \"plant_ncRNA\":\"PlantNonCoding\",\n",
    "\n",
    "    \"plant_complex\":\"Complex\", \n",
    "\n",
    "    \"metabolite\":\"Metabolite\",\n",
    "\n",
    "    \"pathogen_coding\":\"ForeignCoding\",\n",
    "    \"pathogen_noncoding\":\"ForeignNonCoding\",\n",
    "    \n",
    "    \"plant_abstract\":\"PlantAbstract\",\n",
    "    \n",
    "    \"process\":\"Process\", \n",
    "\n",
    "    np.nan:\"Undefined\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update node labels\n",
    "df_components[\"NodeLabel\"] = df_components[\"NodeType\"].apply(lambda x: components_node_type_to_node_label[x])\n",
    "pd.value_counts(df_components['NodeLabel'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_components[df_components[\"NodeLabel\"] == \"Undefined\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_components.sort_values([\"NodeLabel\", \"Family\", \"NodeName\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def only_asci(x):\n",
    "    return \"\".join([character for character in x if character.isascii()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(helpers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_components['species'] = df_components[\"Species\"].apply(lambda x: x.lower())\n",
    "#df_components['observed_species'] = df_components[\"Species\"].apply(helpers.get_second_item)\n",
    "#df_components['also_observed_in'] = df_components[\"Species\"].apply(helpers.rest_of_items)\n",
    "\n",
    "df_components[\"synonyms\"] = df_components[\"synonyms\"].apply(helpers.list_string_to_nice_string)\n",
    "df_components['AdditionalInfo'].fillna('', inplace=True)\n",
    "df_components.loc[df_components['ModelV'].isna(), 'ModelV'] = 'vNA'\n",
    "\n",
    "df_components['GMM_OCD'].fillna('', inplace=True)\n",
    "df_components['ExtDBlink'].fillna('', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_components['species'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_components.loc[df_components['AddedBy'].isna(), 'AddedBy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_components['AddedBy'] = df_components['AddedBy'].apply(lambda x: x.upper())\n",
    "df_components.loc[df_components['AddedBy']=='ZR/MZ', 'AddedBy'] = 'MZ' \n",
    "df_components['AddedBy'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re_ec = \"ec(?:\\:|\\s)?(\\d+(?:\\.(?:\\-|\\d+)){1,3}(?:\\.n\\d+)?)(?:\\s|$|\\]|,|\\.)\"\n",
    "\n",
    "# also use \"AdditionalInfo\", \"NodeDescription\"\n",
    "def get_external_links(row):\n",
    "    dbs_list = []\n",
    "    \n",
    "    ################\n",
    "    if row['Family'] == \"R-gene\":\n",
    "        dbs_list.append(\"invented:unidentified\")\n",
    "    \n",
    "    ################\n",
    "    if row[\"NodeName\"] in [\"X1\", \"X2\", \"X3\", \"X4\"]:\n",
    "        dbs_list.append(\"invented:unidentified\")\n",
    "    \n",
    "    ################\n",
    "    ocd_id = row['GMM_OCD']\n",
    "    if ocd_id:\n",
    "        dbs_list.append(f\"gmm_ocd:{ocd_id.lower()}\")\n",
    "\n",
    "    ################\n",
    "    x = row['ExtDBlink']\n",
    "    x = x.lower()\n",
    "    \n",
    "    pubchem_match = re.findall(\"(?:pubchem:)\\s*(.+?)(?:\\s|$)\", x)\n",
    "    dbs_list += [f\"pubchem:{idf}\" for idf in pubchem_match]\n",
    "    chebi_match = re.findall(\"(?:chebi:)\\s*(.+?)(?:\\s|$)\", x)\n",
    "    dbs_list += [f\"chebi:{idf}\" for idf in chebi_match]\n",
    "\n",
    "    # regex ocd from x as well\n",
    "    ocd_match = re.findall(\"(ocd_all_.+?(?:\\s|$))\", x)\n",
    "    dbs_list += [f\"gmm_ocd:{idf}\" for idf in ocd_match]\n",
    "\n",
    "    go_match = re.findall(\"go:(\\d{7})\", x)\n",
    "    dbs_list += [f\"go:{idf.strip().rstrip(',.')}\" for idf in go_match]     \n",
    "\n",
    "    \n",
    "    doi_match = re.findall(\"https://doi.org/(\\S+)\", x)\n",
    "    dbs_list += [f\"doi:{idf.strip().rstrip(',.')}\" for idf in doi_match]     \n",
    "    \n",
    "    uniprot_match = re.findall(\"UNIPROT:([OPQ][0-9][A-Z0-9]{3}[0-9]|[A-NR-Z][0-9](?:[A-Z][A-Z0-9]{2}[0-9]){1,2})\", x.upper())\n",
    "    dbs_list += [f\"uniprot:{idf}\" for idf in uniprot_match]\n",
    "                 \n",
    "    mirbase_match = re.findall(\"mirbase:(.*)\", x)\n",
    "    dbs_list += [f\"mirbase:{idf}\" for idf in mirbase_match]\n",
    "\n",
    "    conceptual_matches = re.findall(\"(conceptual:(?:process|complex))\", x)\n",
    "    dbs_list += [f\"{idf}\" for idf in conceptual_matches]\n",
    "                 \n",
    "    ################\n",
    "    x = row['AdditionalInfo']\n",
    "    x = x.lower()\n",
    "    \n",
    "    # EC:3.3.3.- EC 2.2.1.7\n",
    "    ec_match = re.findall(re_ec, x)\n",
    "    dbs_list += [f\"ec:{idf}\" for idf in ec_match]\n",
    "    \n",
    "    pubmed_match = re.findall(\"pmid(?:\\:|)\\s*(\\d+)\", x)\n",
    "    dbs_list += [f\"pmid:{idf}\" for idf in pubmed_match]\n",
    "    \n",
    "    pubmedc_match =  re.findall(\"pmcid(?:\\:|)\\s*(pmc\\d+)\", x)\n",
    "    dbs_list += [f\"pmcid:{idf}\" for idf in pubmedc_match]\n",
    "    \n",
    "    doi_match = re.findall(\"doi(?:\\:|\\/)\\s*(.+?)(?:\\s|$|:)\", x)\n",
    "    dbs_list += [f\"doi:{idf.strip().rstrip(',.')}\" for idf in doi_match]    \n",
    "    \n",
    "    kegg_match = re.findall(r\"((?:k|map|ko|ec|rn|ath)\\d{5})\", x)\n",
    "    dbs_list += [f\"kegg:{idf}\" for idf in kegg_match]              \n",
    "    \n",
    "    ncbi_nuccore_match = re.findall(\"NCBI ID: (.+)\", x)\n",
    "    dbs_list += [f\"ncbi_nuccore:{idf}\" for idf in ncbi_nuccore_match]\n",
    "                 \n",
    "    ################                 \n",
    "    x = row['NodeDescription']\n",
    "    x = x.lower()\n",
    "    \n",
    "    # EC:3.3.3.-\n",
    "    ec_match = re.findall(re_ec, x)\n",
    "    dbs_list += [f\"ec:{idf}\" for idf in ec_match]\n",
    "     \n",
    "    kegg_match = re.findall(r\"((?:k|map|ko|ec|rn)\\d{5})\", x)\n",
    "    dbs_list += [f\"kegg:{idf}\" for idf in kegg_match]              \n",
    "    \n",
    "    return ','.join(list(set(dbs_list)))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df_components.apply(get_external_links, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checks\n",
    "for v in ['ETR1', \\\n",
    "          'GST1', \\\n",
    "          'NDB3', \\\n",
    "#           x[df_components['Clade']=='NPH3'], \\\n",
    "          'RBX1B', \\\n",
    "          'DXPS3', \\\n",
    "          'ACX4', \\\n",
    "          'phasiRNA931', \\\n",
    "          'ribosome',  \\\n",
    "          'D53', \\\n",
    "          'miR159b', \\\n",
    "          'Trichome-initiation', \\\n",
    "          #x[:, df_components['NodeName']=='WD/bHLH/MYB']\n",
    "         ]:\n",
    "\n",
    "    print(x[df_components[df_components['NodeName']==v].index[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_components['external_links'] = x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_db(external_links, search_db=\"kegg\"):\n",
    "    if external_links != \"\":\n",
    "        for dbval in external_links.split(\",\"):\n",
    "            try:\n",
    "                db, val = dbval.split(\":\")\n",
    "                if db == search_db:\n",
    "                    return val\n",
    "            except ValueError:\n",
    "                print(\"issue\", dbval)\n",
    "                return \"\"\n",
    "    return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_components['gmm_ocd'] = x.apply(get_db, search_db=\"gmm_ocd\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_components[[\"NodeLabel\", \"Family\", \"Clade\", \"NodeName\",  \"NodeID\", \"synonyms\", \\\n",
    "               \"external_links\", \"gmm_ocd\", \"ExtDBlink\", \"GMM_OCD\", \\\n",
    "               \"AdditionalInfo\", \"NodeDescription\"]].to_csv(os.path.join(\"..\", \"data\", \"parsed\", \"components-lit-check.tsv\"), sep=\"\\t\", index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See https://unicode-table.com/en/ and https://www.utf8-chartable.de/unicode-utf8-table.pl \n",
    "\n",
    "def only_asci(x):\n",
    "    return \"\".join([character for character in x if character.isascii()])\n",
    "\n",
    "def find_non_ascii(x):\n",
    "    x = str(x)\n",
    "    has_nonascii = False\n",
    "    for character in x:\n",
    "        if not character.isascii():\n",
    "            has_nonascii = True\n",
    "            print(character, ord(character), character.encode())\n",
    "    if has_nonascii:\n",
    "        print(x)\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "ascii_replacers = {\n",
    "    b'\\xc2\\xa0'         : b' ',            # funky WIN whitespace\n",
    "    b'\\xe2\\x80\\xa6'     : b'...',          # …\n",
    "    b'\\xe2\\x80\\x8b'     : b'',             # have no figging clue\n",
    "    b'\\xe2\\x80\\x93'     : b'-',            # –\n",
    "    \n",
    "    b'\\xce\\xb1'         : b'&alpha;',      # α\n",
    "    b'\\xc3\\x9f'         : b'&beta;',       # ß\n",
    "    b'\\xce\\xb2'         : b'&beta;',       # β\n",
    "    \n",
    "    # some \"prime\" symbols...\n",
    "    b'\\xe2\\x80\\x98'     : b'&prime;',      # ‘ Left Single Quotation Mark\n",
    "    b'\\xe2\\x80\\x99'     : b'&prime;',      # ’ Right Single Quotation Mark\n",
    "    b'\\xc2\\xb4'         : b'&prime;',      # ´ Acute Accent\n",
    "    # actual prime\n",
    "    b'\\xe2\\x80\\xb2'     : b'&prime;',      # ′ Prime\n",
    "    \n",
    "    # Sorry accents :(\n",
    "    b'\\xc5\\xa0'         : b'S',            # Š\n",
    "    b'\\xc5\\xa1'         : b's',            # š\n",
    "    b'\\xc5\\xbd'         : b'Z',            # Ž\n",
    "    b'\\xc4\\x8d'         : b'c'             # č\n",
    "}\n",
    "\n",
    "\n",
    "def replacer(x, verbose=False):\n",
    "    if type(x) == float:\n",
    "        return x\n",
    "    y = x.encode('utf-8')\n",
    "    for old, new in ascii_replacers.items():\n",
    "        y = y.replace(old, new)\n",
    "    y = y.decode('utf-8')\n",
    "    \n",
    "    if verbose and ( y != x):\n",
    "        print(f\"'{x}' : '{y}'\")\n",
    "    \n",
    "    return y.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_cols = []\n",
    "for c in df_components.columns:\n",
    "    print(c, \"\\n-------------\")\n",
    "    if any(df_components[c].apply(find_non_ascii)):\n",
    "        bad_cols.append(c)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in bad_cols:\n",
    "    print(c, \"\\n-------------\")\n",
    "    df_components[c] = df_components[c].apply(replacer, verbose=True)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in bad_cols:\n",
    "    print(c, \"\\n-------------\")\n",
    "    if any(df_components[c].apply(find_non_ascii)):\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# also need to look for ' (quote) and \" (doube; quote)\n",
    "# which are used instead of prime, and may cause string issues\n",
    "\n",
    "# replacing 5' with 5&prime; and 3' with 3&prime;\n",
    "def find_quotes(x):\n",
    "    x = str(x)\n",
    "    if x.find('\"') != -1: \n",
    "        print(x)\n",
    "        return True\n",
    "    elif x.find(\"'\") != -1:\n",
    "        print(x)\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "        \n",
    "\n",
    "def quote_replacer(x, verbose=False):\n",
    "    if type(x) == float:\n",
    "        x = \"\"\n",
    "    y = x.replace(\"5'\",  \"5&prime;\")\n",
    "    y = y.replace(\"3'\",  \"3&prime;\")\n",
    "\n",
    "    if verbose and ( y != x):\n",
    "        print(f\"'{x}' : '{y}'\")\n",
    "        \n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_cols = []\n",
    "for c in df_components.columns:\n",
    "    print(c, \"\\n-------------\")\n",
    "    if any(df_components[c].apply(find_quotes)):\n",
    "        bad_cols.append(c)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in bad_cols:\n",
    "    print(c, \"\\n-------------\")\n",
    "    df_components[c] = df_components[c].apply(quote_replacer, verbose=True)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in bad_cols:\n",
    "    print(c, \"\\n-------------\")\n",
    "    if any(df_components[c].apply(find_quotes)):\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# duplicated node names\n",
    "for label, subdf in df_components.groupby('NodeLabel'):\n",
    "    dups =  subdf[subdf.duplicated(['NodeName'], keep=False)]\n",
    "    if dups.shape[0] > 0:\n",
    "        print(label)\n",
    "        display(dups.sort_values('NodeName'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_components.loc[df_components['NodeLabel'].isin(['PlantCoding']), 'NodeID'] = df_components.loc[df_components['NodeLabel'].isin(['PlantCoding']), 'NodeID'].str.upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_components.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "want_columns = ['identifier', 'AddedBy', 'species', 'NodeLabel', 'NodeType', \n",
    "                'Family', 'Clade', 'NodeID', 'NodeName', \n",
    "                'external_links', 'NodeDescription', 'AdditionalInfo', \n",
    "                'Process', 'ModelV', 'ModelStatus',                 \n",
    "                'gmm_ocd', 'GMM_Description', 'GMM_ShortName', 'synonyms'\n",
    "               ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path =  output_path / \"components.tsv\"\n",
    "df_components[want_columns].fillna('').to_csv(path, sep=\"\\t\", index=None)\n",
    "print(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head $path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
