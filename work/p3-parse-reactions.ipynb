{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:80% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import neo4j DB: 3/4\n",
    "\n",
    "Code to translate v2.7.4_PIS-model.xlsx to neo4j database. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "import os\n",
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "base_path = Path(\"..\")\n",
    "input_path = base_path / \"data\" / \"raw\"\n",
    "output_path = base_path / \"data\" / \"parsed\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Connect to graph via docker-compose link. See http://localhost:7474/browser/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from py2neo import Graph, Node, Relationship"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = Graph(host=\"neo4j\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in sheets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sheets = [#(file, sheet_name)\n",
    "    (\"v2.7.9_PIS-model.xlsx\", \"Reactions\"),\n",
    "#     (\"v2.7.5_PIS-model.xlsx\", \"Reactions\"),\n",
    "#     (\"v2.7.5_PIS-model.xlsx\", \"Reactions_New\"), \n",
    "#     (\"Model_CK.xlsx\", \"Reactions_new\"), \n",
    "#     (\"v2.7.2_PIS-model-JALR.xlsx\", \"Reactions_New\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resave xlsx as tsv\n",
    "drops = ['FOXMES', 'Legacy:Process', 'Legacy:ReactionMode', \"Comment\"]#, 'ConnID']\n",
    "col_rename = {\n",
    "    'Status':'Status',\n",
    "    'AddedBy':'AddedBy',\n",
    "    'Species':'Species',\n",
    "    'ID':'input1_ID',\n",
    "    'level':'input1_level',\n",
    "    'localisation':'input1_localisation',\n",
    "    'type':'input1_type',\n",
    "    'ID.1':'input2_ID',\n",
    "    'level.1':'input2_level',\n",
    "    'localisation.1':'input2_localisation',\n",
    "    'type.1':'input2_type',\n",
    "    'ID.2':'input3_ID',\n",
    "    'level.2':'input3_level',\n",
    "    'localisation.2':'input3_localisation',\n",
    "    'type.2':'input3_type',\n",
    "    'ReactionEffect':'ReactionEffect',\n",
    "    'ReactionMode':'ReactionMode',\n",
    "    'Modifications':'Modifications',\n",
    "    'ID.3':'output1_ID',\n",
    "    'level.3':'output1_level',\n",
    "    'localisation.3':'output1_localisation',\n",
    "    'type.3':'output1_type',\n",
    "    'TrustLevel':'TrustLevel',\n",
    "    'Literature':'Literature',\n",
    "    'AdditionalInfo':'AdditionalInfo',\n",
    "    'Comment':'Comment',\n",
    "    'Model-v':'ModelV',\n",
    "    'KINETICS':'kinetics', \n",
    "    'ConnID': 'ConnID'\n",
    "}\n",
    "\n",
    "for file_name, sheet_name in sheets:\n",
    "    \n",
    "    print(file_name, sheet_name)\n",
    "    \n",
    "    file_path = input_path / file_name\n",
    "    \n",
    "    base_name, extension = os.path.splitext(file_name)\n",
    "    new_file_path = output_path / f'{base_name}-{sheet_name}.tsv'\n",
    "    \n",
    "    #if os.path.exists(new_file_path):\n",
    "    #    continue\n",
    "        \n",
    "    df = pd.read_excel(file_path, \n",
    "                    sheet_name=sheet_name, \n",
    "                    header=[1], \n",
    "                    dtype=str, \n",
    "                    na_values=helpers.empty_strings)\n",
    "\n",
    "    #df = df[~df[\"AddedBy\"].isna()]\n",
    "    #if 'Status' in df.columns:\n",
    "    #    df = df[df['Status'].isin([\"forCB\", \"forCB_INVENTED\", np.nan])]\n",
    "\n",
    "    to_drop = list(set(drops) & set(df.columns)) + list(df.filter(regex=(\"Unnamed.*\")).columns)\n",
    "    df.drop(to_drop, axis=1, inplace=True)\n",
    "    \n",
    "    new_cols = [col_rename[x] for x in df.columns]\n",
    "    df.columns = new_cols\n",
    "    \n",
    "    df['origin'] = df['Status'] + f'-{base_name}-{sheet_name}'\n",
    "    \n",
    "    df.to_csv(new_file_path, sep=\"\\t\", index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = []\n",
    "\n",
    "for file_name, sheet_name in sheets:\n",
    "    \n",
    "    print(file_name, sheet_name)\n",
    "    base_name, extension = os.path.splitext(file_name)\n",
    "\n",
    "    file_path = output_path / f'{base_name}-{sheet_name}.tsv'\n",
    "    \n",
    "    df = pd.read_csv(file_path, sep=\"\\t\")\n",
    "\n",
    "    dfs.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_edges = pd.concat(dfs, sort=False)\n",
    "df_edges.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_edges.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_edges['ReactionMode'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in df_edges.columns:\n",
    "    df_edges[c] = df_edges[c].str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df_edges[df_edges['AddedBy']=='x'].index\n",
    "print(x)\n",
    "#df_edges.drop(x, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in df_edges[\"TrustLevel\"].unique():\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_edges['trust_level']  = df_edges[\"TrustLevel\"].apply(lambda x: re.search( r\"(R[1|2|3|4|x|y]|undefined)\", x).groups()[0])\n",
    "#df_edges['observed_species'] = df_edges['Species'].apply(helpers.lower_string)\n",
    "#df_edges['species_also_observed_in'] = df_edges[\"Species\"].apply(helpers.rest_of_items)\n",
    "#df_edges['Comment'] = df_edges['Comment'].fillna(\"\")\n",
    "df_edges['AdditionalInfo'] = df_edges['AdditionalInfo'].fillna(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_edges['AddedBy'] = df_edges['AddedBy'].apply(lambda x: x.upper())\n",
    "df_edges[\"AddedBy\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_edges.loc[df_edges['ModelV'].isna(), 'ModelV'] = 'vNA'\n",
    "df_edges['ModelV'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See here for keys https://www.utf8-chartable.de/unicode-utf8-table.pl \n",
    "\n",
    "def only_asci(x):\n",
    "    return \"\".join([character for character in x if character.isascii()])\n",
    "\n",
    "def find_non_ascii(x):\n",
    "    x = str(x)\n",
    "    has_nonascii = False\n",
    "    for character in x:\n",
    "        if not character.isascii():\n",
    "            has_nonascii = True\n",
    "            print('    ', character, ord(character), character.encode())\n",
    "    if has_nonascii:\n",
    "        print(x)\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "ascii_replacers = {\n",
    "    b'\\xc2\\xa0'         : b' ',            # funky WIN whitespace\n",
    "    b'\\xe2\\x80\\xa6'     : b'...',          # …\n",
    "    b'\\xe2\\x80\\x8b'     : b'',             # have no figging clue\n",
    "    b'\\xe2\\x80\\x93'     : b'-',            # –\n",
    "    \n",
    "    b'\\xce\\xb1'         : b'&alpha;',      # α\n",
    "    b'\\xc3\\x9f'         : b'&beta;',       # ß\n",
    "    b'\\xce\\xb2'         : b'&beta;',       # β\n",
    "    \n",
    "    # some \"prime\" symbols...\n",
    "    b'\\xe2\\x80\\x98'     : b'&prime;',      # ‘ Left Single Quotation Mark\n",
    "    b'\\xe2\\x80\\x99'     : b'&prime;',      # ’ Right Single Quotation Mark\n",
    "    b'\\xc2\\xb4'         : b'&prime;',      # ´ Acute Accent\n",
    "    # actual prime\n",
    "    b'\\xe2\\x80\\xb2'     : b'&prime;',      # ′ Prime\n",
    "    \n",
    "    # Sorry accents :(\n",
    "    b'\\xc5\\xa0'         : b'S',            # Š\n",
    "    b'\\xc5\\xa1'         : b's',            # š\n",
    "    b'\\xc5\\xbd'         : b'Z',            # Ž\n",
    "    b'\\xc4\\x8d'         : b'c'             # č\n",
    "}\n",
    "\n",
    "def replacer(x, verbose=False):\n",
    "    if type(x) == float:\n",
    "        return x\n",
    "    y = x.encode('utf-8')\n",
    "    for old, new in ascii_replacers.items():\n",
    "        y = y.replace(old, new)\n",
    "    y = y.decode('utf-8')\n",
    "    \n",
    "    if verbose and ( y != x):\n",
    "        print(f\"'{x}' : '{y}'\")\n",
    "    \n",
    "    return y.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_cols = []\n",
    "for c in df_edges.columns:\n",
    "    print(c, \"\\n-------------\")\n",
    "    if any(df_edges[c].apply(find_non_ascii)):\n",
    "        bad_cols.append(c)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in bad_cols:\n",
    "    print(c, \"\\n-------------\")\n",
    "    df_edges[c] = df_edges[c].apply(replacer, verbose=True)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in bad_cols:\n",
    "    print(c, \"\\n-------------\")\n",
    "    if any(df_edges[c].apply(find_non_ascii)):\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# also need to look for ' (quote) and \" (doube; quote)\n",
    "# which are used instead of prime, (may cause string issues??)\n",
    "\n",
    "# replacing 5' with 5&prime; and 3' with 3&prime;\n",
    "def find_quotes(x):\n",
    "    x = str(x)\n",
    "    if x.find('\"') != -1: \n",
    "        print(x)\n",
    "        return True\n",
    "    elif x.find(\"'\") != -1:\n",
    "        print(x)\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "        \n",
    "\n",
    "def quote_replacer(x, verbose=False):\n",
    "    if type(x) == float:\n",
    "        x = \"\"\n",
    "    y = x.replace(\"5'\", \"5&prime;\")\n",
    "    y = y.replace(\"3'\", \"3&prime;\")\n",
    "\n",
    "    if verbose and ( y != x):\n",
    "        print(f\"'{x}' : '{y}'\")\n",
    "        \n",
    "    return y\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_cols = []\n",
    "for c in df_edges.columns:\n",
    "    print(c, \"\\n-------------\")\n",
    "    if any(df_edges[c].apply(find_quotes)):\n",
    "        bad_cols.append(c)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in bad_cols:\n",
    "    print(c, \"\\n-------------\")\n",
    "    df_edges[c] = df_edges[c].apply(quote_replacer, verbose=True)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in bad_cols:\n",
    "    print(c, \"\\n-------------\")\n",
    "    if any(df_edges[c].apply(find_quotes)):\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_edges[\"Literature\"].fillna(value=\"\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# format literature sources\n",
    "\n",
    "re_ec = \"ec(?:\\:|\\s)?(\\d+(?:\\.(?:\\-|\\d+)){1,3}(?:\\.n\\d+)?)(?:\\s|$|\\]|,|\\.)\"\n",
    "\n",
    "\n",
    "def doi_list(x):\n",
    "    x = x.lower()\n",
    "    match = re.findall(\"(?:doi)(?:\\:|\\/)\\s*(.+?)(?:\\s|$)\", x)\n",
    "    if not match is None:\n",
    "        return [\"doi:\" + m.rstrip('.') for m in match]\n",
    "    else:\n",
    "        return []\n",
    "\n",
    "def pubmed_list(x):\n",
    "    x =x.lower()\n",
    "    match = re.findall(\"(?:pmid)\\:\\s*(.+?)(?:\\s|$)\", x)\n",
    "    if not match is None:\n",
    "        return [\"pmid:\" + m.rstrip('.') for m in match]\n",
    "    else:\n",
    "        return []\n",
    "\n",
    "\n",
    "def format_literature(row):\n",
    "    issued = False\n",
    "    dbs_list = []\n",
    "    \n",
    "    x = row['Literature'].lower().strip()\n",
    "    doi_match = re.findall(\"(?:doi)(?:\\:|\\/)\\s*(.+?)(?:\\s|$)\", x)\n",
    "    dbs_list += [f\"doi:{idf.strip().rstrip('.')}\" for idf in doi_match]    \n",
    "\n",
    "    pubmed_match = re.findall(\"pmid(?:\\:|)\\s*(\\d+)\", x)\n",
    "    dbs_list += [f\"pmid:{idf}\" for idf in pubmed_match]\n",
    "    \n",
    "    pubmedc_match =  re.findall(\"pmcid(?:\\:|)\\s*(pmc\\d+)\", x)\n",
    "    dbs_list += [f\"pmcid:{idf}\" for idf in pubmedc_match]\n",
    "                 \n",
    "    for key in x.split(\"|\"):\n",
    "        if key ==\"\":\n",
    "            print(f\"BLANK\\\\{row['ConnID']}\")\n",
    "            issued = True\n",
    "\n",
    "        elif \":\" in key:\n",
    "            if \"aracyc\" in key:\n",
    "                aracyc_string = \"aracyc:\" + key.split(\":\")[1].strip()\n",
    "                dbs_list.append(aracyc_string)\n",
    "            elif \"kegg\" in key:\n",
    "                kegg_string = \"kegg:\" + key.split(\":\")[1].strip()\n",
    "                dbs_list.append(kegg_string)\n",
    "            elif \"doi\" in key:\n",
    "                # already fetched\n",
    "                continue\n",
    "            elif (\"pmcid\" in key) or (\"pmid\" in key):\n",
    "                continue\n",
    "            else:\n",
    "                print(f\"UNKOWN DB\\\\{row['ConnID']}\\\\{key}\")                 \n",
    "                issued = True\n",
    "        elif \"invented\" in key:\n",
    "            dbs_list.append(\"invented:reason\")\n",
    "        else:\n",
    "            print(f\"NOVALUE\\\\{row['ConnID']}\\\\{key}\")\n",
    "            issued = True\n",
    "\n",
    "    if (len(dbs_list)==0) and not issued:\n",
    "        print(f\"BADorMISSING\\\\{row['ConnID']}\\\\{x}\")\n",
    "\n",
    "    ################\n",
    "    x = row['AdditionalInfo']\n",
    "    x = x.lower()\n",
    "    \n",
    "    # EC:3.3.3.-\n",
    "    ec_match = re.findall(re_ec, x)\n",
    "    dbs_list += [f\"ec:{idf}\" for idf in ec_match]\n",
    "    \n",
    "    pubmed_match = re.findall(\"pmid(?:\\:|)\\s*(\\d+)\", x)\n",
    "    dbs_list += [f\"pmid:{idf}\" for idf in pubmed_match]\n",
    "    \n",
    "    pubmedc_match =  re.findall(\"pmcid(?:\\:|)\\s*(pmc\\d+)\", x)\n",
    "    dbs_list += [f\"pmcid:{idf}\" for idf in pubmedc_match]\n",
    "    \n",
    "    doi_match = re.findall(\"doi(?:\\:|\\/)\\s*(.+?)(?:\\s|$)\", x)\n",
    "    dbs_list += [f\"doi:{idf.strip().rstrip('.')}\" for idf in doi_match]    \n",
    "    \n",
    "    kegg_match = re.findall(r\"((?:k|map|ko|ec|rn)\\d{5})\", x)\n",
    "    dbs_list += [f\"kegg:{idf}\" for idf in kegg_match]              \n",
    "    \n",
    "    ncbi_nuccore_match = re.findall(\"NCBI ID: (.+)\", x)\n",
    "    dbs_list += [f\"ncbi_nuccore:{idf}\" for idf in ncbi_nuccore_match]              \n",
    "         \n",
    "    return ','.join(list(set(dbs_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_edges['external_links'] = df_edges.apply(format_literature, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_edges[df_edges['external_links']==''][['ConnID', 'origin', 'Literature', 'AdditionalInfo']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_edges[[\"ConnID\",  \"external_links\", \"Literature\", \"AdditionalInfo\"]].to_csv(output_path / \"reactions-lit-check.tsv\", sep=\"\\t\", index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_edges.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_df = df_edges.copy()\n",
    "#df_edges = save_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_edges.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in ['input1', 'input2', 'input3', 'output1']:\n",
    "    df_edges.loc[:, x + \"_ID\"] = df_edges[x + \"_ID\"].apply(helpers.reorder_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_type_to_node_form_dict = {\n",
    "    \"gene\":\"gene\",\n",
    "    \n",
    "    \"protein\":\"protein\",\n",
    "    \"protein [inactivated]\":\"protein\", \n",
    "    \"protein [activated]\":\"protein_active\",\n",
    "    'protein [active]': \"protein_active\",\n",
    "    'protein_active': 'protein_active',\n",
    "    \n",
    "    \"ncRNA\":\"ncRNA\",\n",
    "    \"plant_ncRNA\":\"ncRNA\",\n",
    "    'ta-siRNA':\"ta-siRNA\", \n",
    "        \n",
    "    \"complex\":\"complex\", \n",
    "    \"plant_complex\":\"complex\",\n",
    "    'complex [active]': \"complex_active\",\n",
    "    'complex_active': \"complex_active\",\n",
    "    \n",
    "    \"metabolite\":\"metabolite\",\n",
    "    \n",
    "    \"process\":\"process\", \n",
    "    \"process_active\":\"process_active\", \n",
    "    'process [active]':\"process_active\",\n",
    "\n",
    "    np.nan:\"\", \n",
    "}\n",
    "\n",
    "node_type_to_active_form_dict = {\n",
    "    \"protein [active]\":\"protein_active\",\n",
    "    \"protein\":\"protein_active\",\n",
    "    \"protein_active\":\"protein_active\",\n",
    "    \n",
    "    \"complex\":\"complex_active\", \n",
    "    \"complex_active\": \"complex_active\",\n",
    "    \n",
    "    \"process\":\"process_active\", \n",
    "    'process_active':\"process_active\",\n",
    "}\n",
    "\n",
    "node_type_to_inactive_form_dict = {\n",
    "    \"protein\":\"protein\",\n",
    "    \"protein_active\":\"protein\",\n",
    "    \"protein [inactivated]\":\"protein\",\n",
    "    \n",
    "    \"complex\":\"complex\", \n",
    "    \"complex_active\": \"complex\",\n",
    "    \n",
    "    \"process\":\"process\", \n",
    "    'process_active':\"process\",\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "def get_node_form(row, prefix=\"input1\"):\n",
    "    id_ = row.iloc[0]\n",
    "    if type(id_) == float:\n",
    "        return np.nan\n",
    "    \n",
    "    type_ = row.iloc[1]\n",
    "    form = node_type_to_node_form_dict[type_]\n",
    "    if (  len(re.findall(\"\\(a\\)\", id_)) > 0  ) or (  len(re.findall(\"\\[active\\]\", id_)) > 0  ):\n",
    "        form = node_type_to_active_form_dict[type_]\n",
    "        print(id_, type_, \"-->\", form)\n",
    "        \n",
    "    return form\n",
    "\n",
    "for prefix in ['input1', 'input2', 'input3', 'output1']:\n",
    "    id_col, type_col, new_form_col  =\\\n",
    "        [prefix + x for x in ('_ID',  '_type',  '_form')]\n",
    "    \n",
    "    #x = df_edges[[id_col, type_col]].dropna(how='all')\n",
    "    \n",
    "    print(prefix)\n",
    "    df_edges[new_form_col] = df_edges[[id_col, type_col]].apply(get_node_form, prefix=prefix, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_brackets(x):\n",
    "    # reorder ids for complexes\n",
    "    if type(x) == np.float:\n",
    "        return np.nan\n",
    "    else:\n",
    "        s = re.sub(\"\\(a\\)\", \"\", x)\n",
    "        s = re.sub(\"\\(p\\)\", \"\", s)\n",
    "        if s != x:\n",
    "            print(x, s)\n",
    "        return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in ['input1', 'input2', 'input3', 'output1']:\n",
    "    df_edges.loc[:, x + \"_ID\"] = df_edges[x + \"_ID\"].apply(remove_brackets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_edges['species'] = df_edges['Species'].apply(lambda x: \",\".join(x.lower().split('/')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_edges['species'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_localisation_dict = {\n",
    "    'nuc':'nucleus',\n",
    "    'er':'endoplasmic reticulum',\n",
    "    'golgi':'golgi apparatus', \n",
    "    'mitochondria?': 'putative:mitochondrion', \n",
    "    'cytoplasm?': 'putative:cytoplasm', \n",
    "}\n",
    "\n",
    "\n",
    "good_localisations = set([\n",
    "     'nucleus',\n",
    "     'nucleolus',\n",
    "     'cytoplasm',\n",
    "     'vacuole',\n",
    "     'endoplasmic reticulum',\n",
    "     'chloroplast',\n",
    "     'mitochondrion',\n",
    "     'golgi apparatus',\n",
    "     'peroxisome',\n",
    "     'apoplast',\n",
    "     'extracellular'\n",
    "])\n",
    "\n",
    "good_localisations.update(['putative:' + s for s in good_localisations])\n",
    "\n",
    "\n",
    "def node_localisation_std(x):\n",
    "    if not type(x) == str:\n",
    "        return \"putative:cytoplasm\"\n",
    "    \n",
    "    x = x.lower()\n",
    "    if x in node_localisation_dict.keys():\n",
    "        x = node_localisation_dict[x]\n",
    "    \n",
    "    if x in good_localisations:\n",
    "        return x\n",
    "    else:\n",
    "        print(x)\n",
    "        return \"\"\n",
    "\n",
    "node_localisations = set()\n",
    "new_localisation = set()\n",
    "for prefix in ['input1', 'input2', 'input3', 'output1']:\n",
    "    id_col, type_col, localisation_col, location_col  =\\\n",
    "        [prefix + x for x in ('_ID',  '_type',  '_localisation', '_location')]\n",
    "    \n",
    "    x = df_edges[['ConnID', 'origin', id_col, localisation_col]].dropna(how='all', subset=[ id_col, localisation_col])\n",
    "    for _, y in x.iterrows():\n",
    "        if (not (y[id_col] in helpers.empty_strings)) and (y[localisation_col] in helpers.empty_strings + ['']):\n",
    "            print(y['origin'], \"|\", y['ConnID'], \"|\", prefix, \"|\", y[id_col], \"|\", y[localisation_col])\n",
    "    \n",
    "    \n",
    "    node_localisations.update(x[localisation_col])\n",
    "    \n",
    "    #print(prefix)\n",
    "    df_edges[location_col] = df_edges[localisation_col].apply(node_localisation_std)\n",
    "    \n",
    "    new_localisation.update(df_edges[location_col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_localisations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_localisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_localisations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('reaction_mode_dict = {')\n",
    "for s in df_edges['ReactionMode'].unique():\n",
    "    print(f\"\\t'{s}':'{'/'.join([x.lower().strip() for x in str(s).split('/')])}',\")\n",
    "print('}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Old version before edit\n",
    "# reaction_mode_dict = {\n",
    "# \t'catalysis / auto-catalysis':'catalysis/auto-catalysis',\n",
    "# \t'translocation':'translocation',\n",
    "# \t'protein activation':'protein activation',\n",
    "# \t'binding / oligomerisation':'binding/oligomerisation',\n",
    "# \t'protein deactivation':'protein deactivation',\n",
    "# \t'transcription/translation repression':'transcription/translation repression',\n",
    "# \t'transcription/translation induction':'transcription/translation induction',\n",
    "# \t'degradation / secretion':'degradation/secretion',\n",
    "# \t'dissociation':'dissociation',\n",
    "# \t'translation':'translation',\n",
    "# \t'transcription':'transcription',\n",
    "# \t'by binding':'by binding',\n",
    "# \t'nan':'nan',\n",
    "# \t'cleavage / auto-cleavage':'cleavage/auto-cleavage',\n",
    "# \t'binding/oligomerisation':'binding/oligomerisation',\n",
    "# \t'protein phosphorylation':'protein phosphorylation',\n",
    "# }\n",
    "\n",
    "# Old version after edit\n",
    "# reaction_mode_dict = {\n",
    "# \t'catalysis / auto-catalysis':'catalysis/auto-catalysis',\n",
    "# \t'translocation':'translocation',\n",
    "# \t'protein activation':'protein activation',\n",
    "# \t'binding / oligomerisation':'binding/oligomerisation',\n",
    "# \t'protein deactivation':'protein deactivation',\n",
    "# \t'transcription/translation repression':'transcription/translation repression',\n",
    "# \t'transcription/translation induction':'transcription/translation induction',\n",
    "# \t'degradation / secretion':'degradation/secretion',\n",
    "# \t'dissociation':'dissociation',\n",
    "# \t'translation':'bad',\n",
    "# \t'transcription':'bad',\n",
    "# \t'by binding':'bad',\n",
    "# \tnp.nan:'bad',\n",
    "# \t'cleavage / auto-cleavage':'bad',\n",
    "# \t'binding/oligomerisation':'binding/oligomerisation',\n",
    "# \t'protein phosphorylation':'bad',\n",
    "# }\n",
    "\n",
    "reaction_mode_dict = {\n",
    "\t'catalysis / auto-catalysis':'catalysis',\n",
    "\t'translocation':'translocation',\n",
    "\t'protein activation':'protein activation',\n",
    "\t'binding / oligomerisation':'binding/oligomerisation',\n",
    "\t'protein deactivation':'protein deactivation',\n",
    "\t'transcriptional / translational repression':'transcriptional/translational repression',\n",
    "\t'transcriptional / translational induction':'transcriptional/translational activation',\n",
    "\t'degradation / secretion':'degradation/secretion',\n",
    "\tnp.nan:'undefined',\n",
    "\t'dissociation':'dissociation',\n",
    "\t'cleavage / auto-cleavage':'cleavage/auto-cleavage',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_edges['reaction_type'] = df_edges['ReactionMode'].apply(lambda x: reaction_mode_dict[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_edges[df_edges['reaction_type']=='undefined'][['origin', 'ConnID', 'ReactionMode', 'ReactionEffect', 'reaction_type']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_edges.to_csv(\"parsed_reactions.tsv\", sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# END"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
